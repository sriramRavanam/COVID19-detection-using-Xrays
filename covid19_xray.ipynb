{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "xrayfinal.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "pykQOm6dUieh"
      },
      "source": [
        "import zipfile\n",
        "#!unrar x \"/content/Final_data.rar\" \"/content/\"\n",
        "\n",
        "!unrar x \"/content/drive/MyDrive/Final_data.rar\" \"/content/\"\n",
        "\n",
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "from imutils import paths\n",
        "import cv2\n",
        "import random\n",
        "from sklearn.model_selection import train_test_split\n",
        "from tensorflow.keras.layers import Input\n",
        "from tensorflow.keras.models import Model\n",
        "from sklearn.metrics import classification_report\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "\n",
        "\n",
        "#to open and load images\n",
        "pos_im_paths = list(paths.list_images(\"/content/Final_data/final_Positives\"))\n",
        "neg_im_paths = list(paths.list_images(\"/content/Final_data/final_Negatives\"))\n",
        "\n",
        "#add labels to data\n",
        "data = []\n",
        "labels = []\n",
        "\n",
        "for imPath in pos_im_paths:\n",
        "  image = cv2.imread(imPath)\n",
        "  image = cv2.cvtColor(image,cv2.COLOR_BGR2RGB)\n",
        "  image = cv2.resize(image,(224,224))\n",
        "\n",
        "  data.append(image)\n",
        "  labels.append(1)\n",
        "\n",
        "for imPath in neg_im_paths:\n",
        "  image = cv2.imread(imPath)\n",
        "  image = cv2.cvtColor(image,cv2.COLOR_BGR2RGB)\n",
        "  image = cv2.resize(image,(224,224))\n",
        "\n",
        "  data.append(image)\n",
        "  labels.append(0)\n",
        "\n",
        "\n",
        "#shuffle labels and data together\n",
        "temp = list(zip(data, labels)) \n",
        "random.shuffle(temp) \n",
        "data, labels = zip(*temp) \n",
        "\n",
        "data = np.array(data) / 255\n",
        "labels = np.array(labels)\n",
        "\n",
        "\n",
        "#HYPER Paramaters\n",
        "INIT_LR = 1e-3\n",
        "EPOCHS = 12\n",
        "BS = 155\n",
        "\n",
        "#some hyperparameters\n",
        "LR = [1e-3 , 3e-3,3e-4,3e-2]\n",
        "bs = [155,70]\n",
        "\n",
        "#data split and data augmentation\n",
        "\n",
        "(x_train, x_test, y_train, y_test) = train_test_split(data,labels,test_size = 0.20,stratify = labels)\n",
        "\n",
        "#convert labels into one hot vectors\n",
        "y_train_Ohot = tf.one_hot(y_train,2)\n",
        "y_test_Ohot = tf.one_hot(y_test,2)\n",
        "\n",
        "trainAug = tf.keras.preprocessing.image.ImageDataGenerator(\n",
        "\trotation_range=15,\n",
        "\tfill_mode=\"nearest\")\n",
        "\n",
        "\n",
        "#creating model\n",
        "\n",
        "base = tf.keras.applications.VGG16(weights=\"imagenet\",include_top=False,input_tensor=Input(shape =(224,224,3)))\n",
        "\n",
        "hmodel = base.output\n",
        "\n",
        "hmodel = tf.keras.layers.AveragePooling2D(pool_size=(4,4),strides = 2)(hmodel)\n",
        "hmodel = tf.keras.layers.Flatten(name = \"flatten\")(hmodel)\n",
        "hmodel = tf.keras.layers.Dense(64,activation=\"relu\")(hmodel)\n",
        "hmodel = tf.keras.layers.Dropout(0.2)(hmodel)\n",
        "hmodel = tf.keras.layers.Dense(16)(hmodel)\n",
        "hmodel = tf.keras.layers.LeakyReLU()(hmodel)\n",
        "hmodel = tf.keras.layers.Dense(2,activation = \"softmax\")(hmodel)\n",
        "\n",
        "stack = Model(inputs = base.input , outputs = hmodel)\n",
        "\n",
        "\n",
        "for layer in base.layers:\n",
        "  layer.trainable = False\n",
        "\n",
        "\n",
        "optimizer = tf.keras.optimizers.Adam(learning_rate=INIT_LR,decay=INIT_LR / EPOCHS)\n",
        "stack.compile(loss = \"binary_crossentropy\",optimizer = optimizer,metrics = [\"accuracy\"])\n",
        "\n",
        "H = stack.fit(\n",
        "\ttrainAug.flow(x_train, y_train_Ohot), batch_size = BS,\n",
        "\tepochs=EPOCHS)\n",
        "\n",
        "p,q = stack.evaluate(x_test,y_test_Ohot)\n",
        "\n",
        "predictions = stack.predict(x_test,batch_size=BS) #h(x)\n",
        "\n",
        "pred = np.argmax(predictions,axis=1)\n",
        "ynp = np.array(y_test_Ohot)#y\n",
        "\n",
        "stack.summary()\n",
        "\n",
        "#hyperparameter testing, learning rate and batch size against accuracy and loss\n",
        "\n",
        "class Results:\n",
        "  def __init__(self,lr,bat,l,a):\n",
        "    self.l=l\n",
        "    self.a=a\n",
        "    self.lr=lr\n",
        "    self.bat=bat\n",
        "\n",
        "\n",
        "#refitting the model with new hyperparameters\n",
        "res = []\n",
        "for i in LR:\n",
        "  for j in bs:\n",
        "    optimizer = tf.keras.optimizers.Adam(learning_rate=i,decay=i / EPOCHS)\n",
        "    stack.compile(loss = \"binary_crossentropy\",optimizer = optimizer,metrics = [\"accuracy\"])\n",
        "    stack.fit(\n",
        "\ttrainAug.flow(x_train, y_train_Ohot), batch_size = j,\n",
        "\tepochs=EPOCHS)\n",
        "    p,q = stack.evaluate(x_test,y_test_Ohot)\n",
        "    res.append(Results(i,j,p,q))\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "#Writing into file\n",
        "\n",
        "F = open(\"results.txt\",\"w\")\n",
        "\n",
        "for R in range(len(res)):\n",
        "  F.write(str(res[R].lr)+\"\\t\"+str(res[R].bat)+\"\\t\"+str(res[R].l)+\"\\t\"+str(res[R].a)+\"\\n\")\n",
        "F.close()\n",
        "\n",
        "\n",
        "\n",
        "#read from results.txt and store values in x_float\n",
        "\n",
        "F = open(\"results.txt\",\"r\")\n",
        "x = F.readlines()\n",
        "\n",
        "x_float = []\n",
        "for i in x:\n",
        "  x_float.append(list(map(float,i.replace(\"\\n\",\"\").split(\"\\t\"))))\n",
        "\n",
        "\n",
        "#store values in lists\n",
        "\n",
        "losses = []\n",
        "bat_siz = []\n",
        "lr_rate = []\n",
        "accuracy = []\n",
        "\n",
        "for i in x_float:\n",
        "  losses.append(i[2])\n",
        "  bat_siz.append(i[1])\n",
        "  lr_rate.append(i[0])\n",
        "  accuracy.append(i[3])\n",
        "\n",
        "\n",
        "#loss vs batch_size\n",
        "\n",
        "plt.plot(bat_siz,losses,\"go\",)\n",
        "plt.axis([0,160,0,0.2])\n",
        "plt.xlabel(\"batch_size\")\n",
        "plt.ylabel(\"loss\")\n",
        "\n",
        "\n",
        "\n",
        "#loss vs learning_rate\n",
        "\n",
        "plt.plot(lr_rate,losses,\"bo\",)\n",
        "plt.axis([0.00003,0.04,0,0.2])\n",
        "plt.xlabel(\"learning_rate\")\n",
        "plt.ylabel(\"loss\")\n",
        "\n",
        "\n",
        "#accuracy vs batch_size\n",
        "\n",
        "plt.plot(bat_siz,accuracy,\"g^\",)\n",
        "plt.axis([0,160,0,1])\n",
        "plt.xlabel(\"batch_size\")\n",
        "plt.ylabel(\"accuracy\")\n",
        "\n",
        "#accuracy vs learning_rate \n",
        "\n",
        "plt.plot(lr_rate,accuracy,\"b^\",)\n",
        "plt.axis([0.00003,0.04,0,1])\n",
        "plt.xlabel(\"learning_rate\")\n",
        "plt.ylabel(\"accuracy\")\n",
        "\n",
        "\n",
        "#plot Loss/Accuracy vs Epochs\n",
        "\n",
        "N = EPOCHS\n",
        "plt.style.use(\"ggplot\")\n",
        "plt.figure()\n",
        "plt.plot(np.arange(0, N), H.history[\"loss\"], label=\"train_loss\")\n",
        "plt.plot(np.arange(0, N), H.history[\"accuracy\"], label=\"train_acc\")\n",
        "plt.title(\"Training Loss and Accuracy on COVID-19 Dataset\")\n",
        "plt.xlabel(\"Epoch #\")\n",
        "plt.ylabel(\"Loss/Accuracy\")\n",
        "plt.legend(loc=\"lower left\")\n",
        "\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}